{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b6cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kilua\\AppData\\Local\\Temp\\ipykernel_29936\\920301165.py:3: DtypeWarning: Columns (9,22,31,33,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"subset_dataset_cs2cd.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"subset_dataset_cs2cd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86f8f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9437218, 42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fb5ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'Y', 'Z', 'tick', 'steamid', 'velocity_X', 'velocity_Y',\n",
       "       'velocity_Z', 'is_airborne', 'is_walking', 'yaw', 'pitch',\n",
       "       'usercmd_mouse_dx', 'usercmd_mouse_dy', 'usercmd_viewangle_x',\n",
       "       'usercmd_viewangle_y', 'aim_punch_angle', 'i_recoil_idx',\n",
       "       'fl_recoil_idx', 'active_weapon', 'active_weapon_ammo',\n",
       "       'total_ammo_left', 'FIRE', 'last_shot_time', 'next_primary_attack_tick',\n",
       "       'shots_fired', 'last_shot_time.1', 'kills_total', 'assists_total',\n",
       "       'damage_total', 'headshot_kills_total', 'spotted',\n",
       "       'approximate_spotted_by', 'is_scoped', 'health', 'armor_value',\n",
       "       'is_alive', 'is_cheater', 'map', 'server', 'avg_rank',\n",
       "       'match_making_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"--- PASO 1: Carga Eficiente de Datos ---\")\n",
    "\n",
    "filename = 'subset_dataset_cs2cd.csv'\n",
    "\n",
    "columns_to_load = [\n",
    "    'aim_punch_angle', 'spotted', 'approximate_spotted_by',\n",
    "    'X', 'Y', 'Z', 'velocity_X', 'velocity_Y', 'velocity_Z', 'is_airborne',\n",
    "    'is_walking', 'yaw', 'pitch', 'usercmd_mouse_dx', 'usercmd_mouse_dy',\n",
    "    'usercmd_viewangle_x', 'usercmd_viewangle_y', 'i_recoil_idx',\n",
    "    'fl_recoil_idx', 'active_weapon_ammo', 'total_ammo_left',\n",
    "    'FIRE', 'last_shot_time', 'next_primary_attack_tick', 'shots_fired',\n",
    "    'kills_total', 'assists_total', 'damage_total', 'headshot_kills_total',\n",
    "    'is_scoped', 'health', 'armor_value', 'is_alive', 'avg_rank',\n",
    "    'map', 'match_making_type',\n",
    "    'is_cheater'\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(f\"Cargando datos desde '{filename}'...\")\n",
    "    df = pd.read_csv(filename, usecols=lambda c: c in columns_to_load, low_memory=False)\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "    print(f\"Dimensiones del DataFrame: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo '{filename}'. Asegúrate de que el nombre y la ruta son correctos.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar los datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- PASO 2: Preprocesamiento y Optimización ---\")\n",
    "\n",
    "df_model = df\n",
    "\n",
    "print(\"Limpiando y convirtiendo tipos de dato iniciales...\")\n",
    "for col in ['is_airborne', 'is_alive']:\n",
    "    if col in df_model.columns:\n",
    "        df_model[col] = df_model[col].astype(int)\n",
    "\n",
    "for col in ['is_walking', 'is_scoped']:\n",
    "    if col in df_model.columns:\n",
    "        df_model.loc[:, col] = df_model[col].map({'True': 1, 'False': 0, True: 1, False: 0}).fillna(0)\n",
    "\n",
    "print(\"Buscando y expandiendo columnas de vectores...\")\n",
    "vector_like_columns = [col for col in df_model.columns if df_model[col].dtype == 'object' and df_model[col].astype(str).str.contains(r'\\[', regex=True, na=False).any()]\n",
    "\n",
    "for col_name in vector_like_columns:\n",
    "    print(f\"Procesando y expandiendo '{col_name}'...\")\n",
    "    extracted = df_model[col_name].fillna('').astype(str).str.findall(r'(-?\\d+\\.?\\d*e?-?\\d+)')\n",
    "    temp_df = pd.DataFrame(extracted.tolist(), index=df_model.index).astype(float)\n",
    "    if not temp_df.empty:\n",
    "        num_components = temp_df.shape[1]\n",
    "        temp_df.columns = [f\"{col_name}_{i}\" for i in range(num_components)]\n",
    "        df_model = df_model.join(temp_df)\n",
    "    df_model.drop(columns=[col_name], inplace=True)\n",
    "\n",
    "print(\"Codificando variables categóricas...\")\n",
    "categorical_features = [col for col in df_model.columns if df_model[col].dtype == 'object' or isinstance(df_model[col].dtype, pd.CategoricalDtype)]\n",
    "if categorical_features:\n",
    "    df_model = pd.get_dummies(df_model, columns=categorical_features, drop_first=True, dummy_na=True)\n",
    "\n",
    "print(\"Rellenando valores nulos restantes...\")\n",
    "for col in df_model.select_dtypes(include=np.number).columns:\n",
    "    if df_model[col].isnull().any():\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "print(\"Reduciendo precisión de floats a float32...\")\n",
    "for col in df_model.select_dtypes(include=['float64']).columns:\n",
    "    df_model[col] = df_model[col].astype('float32')\n",
    "\n",
    "print(\"Preprocesamiento completado.\")\n",
    "print(f\"Dimensiones finales del DataFrame para el modelo: {df_model.shape}\")\n",
    "\n",
    "print(\"\\n--- PASO 3: División y Escalado de Datos ---\")\n",
    "X = df_model.drop(columns=['is_cheater'])\n",
    "y = df_model['is_cheater']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Escalando datos... (esto puede tardar debido al tamaño)\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n--- PASO 4: Entrenamiento del Modelo Random Forest ---\")\n",
    "modelo_basico_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    max_depth=20\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "modelo_basico_rf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Entrenamiento completado en {((end_time - start_time) / 60):.2f} minutos.\")\n",
    "\n",
    "print(\"\\n--- PASO 5: Evaluación del Rendimiento ---\")\n",
    "predicciones = modelo_basico_rf.predict(X_test)\n",
    "print(\"\\n--- Reporte de Clasificación del Modelo Básico (Random Forest) ---\")\n",
    "print(classification_report(y_test, predicciones, target_names=['Legítimo (0)', 'Tramposo (1)']))\n",
    "print(\"\\n--- Matriz de Confusión ---\")\n",
    "print(confusion_matrix(y_test, predicciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303635d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paso 1: Definiendo columnas a cargar...\n",
      "Cargando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kilua\\AppData\\Local\\Temp\\ipykernel_28488\\2892000779.py:32: DtypeWarning: Columns (9,22,29,31,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('subset_cs2cd.csv', usecols=lambda c: c in columns_to_load)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente.\n",
      "\n",
      "Paso 2: Preprocesando y optimizando...\n",
      "Reduciendo precisión de floats a float32 para ahorrar memoria...\n",
      "Escalando datos... (esto puede tardar)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.8 GiB for an array with shape (70, 24573374) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEscalando datos... (esto puede tardar)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 85\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocesamiento completado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:2152\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m   2150\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2152\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\n\u001b[0;32m   2153\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2155\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2158\u001b[0m     ):\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:1127\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[0;32m   1129\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12594\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.8 GiB for an array with shape (70, 24573374) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "# Para asegurar la reproducibilidad\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 1: CARGA EFICIENTE DE DATOS (Sin cambios)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 1: Definiendo columnas a cargar...\")\n",
    "columns_to_load = [\n",
    "    'velocity', 'position', 'aim_punch_angle', 'usercmd_viewangle_x', 'usercmd_viewangle_y',\n",
    "    'is_airborne', 'is_walking', 'yaw', 'pitch', 'usercmd_mouse_dx', 'usercmd_mouse_dy',\n",
    "    'i_recoil_idx', 'fl_recoil_idx', 'active_weapon_ammo', 'total_ammo_left',\n",
    "    'FIRE', 'last_shot_time', 'kills_total', 'assists_total', 'damage_total',\n",
    "    'headshot_kills_total', 'spotted', 'approximate_spotted_by', 'is_scoped',\n",
    "    'health', 'armor_value', 'is_alive', 'avg_rank',\n",
    "    'active_weapon_name', 'map', 'match_making_type',\n",
    "    'is_cheater'\n",
    "]\n",
    "try:\n",
    "    print(\"Cargando datos...\")\n",
    "    df = pd.read_csv('subset_cs2cd.csv', usecols=lambda c: c in columns_to_load)\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Reemplaza 'tu_archivo.csv' con el nombre de tu archivo de datos.\")\n",
    "    exit()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 2: PREPROCESAMIENTO Y OPTIMIZACIÓN (CON CORRECCIONES)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 2: Preprocesando y optimizando...\")\n",
    "df_model = df\n",
    "\n",
    "# --- A. Optimización de Tipos de Dato Categóricos ---\n",
    "for col in df_model.select_dtypes(include=['object']).columns:\n",
    "    if not df_model[col].astype(str).str.contains(r'\\[', regex=True, na=False).any():\n",
    "        df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# --- B. Parseo y Expansión de Vectores ---\n",
    "vector_like_columns = [col for col in df_model.columns if df_model[col].dtype == 'object']\n",
    "for col_name in vector_like_columns:\n",
    "    extracted = df_model[col_name].fillna('').astype(str).str.findall(r'(-?\\d+\\.?\\d*e?-?\\d+)')\n",
    "    temp_df = pd.DataFrame(extracted.tolist(), index=df_model.index).astype(float)\n",
    "    if not temp_df.empty:\n",
    "        num_components = temp_df.shape[1]\n",
    "        temp_df.columns = [f\"{col_name}_{i}\" for i in range(num_components)]\n",
    "        df_model = df_model.join(temp_df)\n",
    "    df_model.drop(columns=[col_name], inplace=True)\n",
    "\n",
    "# --- C. Definición Final y Limpieza ---\n",
    "target = 'is_cheater'\n",
    "numerical_features = [col for col in df_model.columns if pd.api.types.is_numeric_dtype(df_model[col].dtype) and col != target]\n",
    "categorical_features = [col for col in df_model.columns if isinstance(df_model[col].dtype, pd.CategoricalDtype)]\n",
    "\n",
    "# Corregir el FutureWarning:\n",
    "for col in numerical_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        # Forma correcta y explícita de rellenar NaNs, sin 'inplace=True'\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "if categorical_features:\n",
    "    df_model = pd.get_dummies(df_model, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# --- D. Reducción de Precisión para Ahorrar Memoria (NUEVO PASO CLAVE) ---\n",
    "print(\"Reduciendo precisión de floats a float32 para ahorrar memoria...\")\n",
    "for col in df_model.select_dtypes(include=['float64']).columns:\n",
    "    df_model[col] = df_model[col].astype('float32')\n",
    "\n",
    "# --- E. Separación y Escalado ---\n",
    "X = df_model.drop(columns=[target])\n",
    "y = df_model[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Escalando datos... (esto puede tardar)\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Preprocesamiento completado.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 3: CONSTRUCCIÓN Y ENTRENAMIENTO DEL MODELO (Sin cambios)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 3: Construyendo y entrenando el modelo...\")\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=1024,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1,\n",
    "                    callbacks=[reduce_lr])\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 4: EVALUACIÓN DEL MODELO (Sin cambios)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 4: Evaluando el rendimiento del modelo...\")\n",
    "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "predictions_prob = model.predict(X_test)\n",
    "binary_predictions = (predictions_prob > 0.5).astype(int)\n",
    "print(f'\\nResultados en el conjunto de prueba:')\n",
    "print(f'Pérdida (Loss): {loss:.4f}')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, binary_predictions))\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f2b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paso 1: Definiendo columnas a cargar y cargando datos eficientemente...\n",
      "Error: Reemplaza 'tu_archivo.csv' con el nombre de tu archivo de datos.\n",
      "\n",
      "Paso 2: Optimizando tipos de datos y preprocesando...\n",
      "Convirtiendo 'steamid' a tipo 'category'...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 85\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# --- A. Optimización de Tipos de Dato Categóricos ---\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Convertir columnas de texto a 'category' es una de las mayores ganancias de memoria.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_model\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Solo convertimos a categoría si no es una columna de vector que vamos a parsear\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdf_model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvirtiendo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m a tipo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m         df_model[col] \u001b[38;5;241m=\u001b[39m df_model[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\strings\\accessor.py:191\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\strings\\accessor.py:245\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    242\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "# Para asegurar la reproducibilidad de los resultados\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 1: CARGA EFICIENTE DE DATOS\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 1: Definiendo columnas a cargar y cargando datos eficientemente...\")\n",
    "\n",
    "# Define de antemano SÓLO las columnas que realmente necesitas del archivo CSV.\n",
    "# Esto reduce drásticamente el uso de memoria al cargar.\n",
    "# NOTA: Asegúrate de que los nombres de las columnas que contienen vectores\n",
    "#       (como 'velocity', 'position', etc.) estén en esta lista.\n",
    "columns_to_load = [\n",
    "    # Columnas que sospechamos son vectores (ejemplos, ajusta a tu archivo)\n",
    "    'velocity', 'position', 'aim_punch_angle',\n",
    "\n",
    "    # Columnas numéricas directas\n",
    "    'is_airborne', 'is_walking', 'yaw', 'pitch', 'usercmd_mouse_dx', 'usercmd_mouse_dy',\n",
    "    'i_recoil_idx', 'fl_recoil_idx', 'active_weapon_ammo', 'total_ammo_left',\n",
    "    'FIRE', 'last_shot_time', 'kills_total', 'assists_total', 'damage_total',\n",
    "    'headshot_kills_total', 'spotted', 'approximate_spotted_by', 'is_scoped',\n",
    "    'health', 'armor_value', 'is_alive', 'avg_rank',\n",
    "\n",
    "    # Columnas categóricas\n",
    "    'map', 'match_making_type',\n",
    "\n",
    "    # La variable objetivo\n",
    "    'is_cheater'\n",
    "]\n",
    "\n",
    "# Carga el CSV usando solo las columnas necesarias para ahorrar memoria\n",
    "# Reemplaza 'tu_archivo.csv' con el nombre real de tu archivo\n",
    "try:\n",
    "    df = pd.read_csv('tu_archivo.csv', usecols=lambda c: c in columns_to_load)\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Reemplaza 'tu_archivo.csv' con el nombre de tu archivo de datos.\")\n",
    "    # Salir si el archivo no se encuentra, o manejar como prefieras\n",
    "    exit()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 2: OPTIMIZACIÓN DE MEMORIA Y PREPROCESAMIENTO\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 2: Optimizando tipos de datos y preprocesando...\")\n",
    "\n",
    "# NO HACEMOS COPIA: df_model ahora apunta al mismo objeto que df\n",
    "# Esto es crucial para no duplicar el uso de memoria.\n",
    "df_model = df\n",
    "\n",
    "# --- A. Optimización de Tipos de Dato Categóricos ---\n",
    "# Convertir columnas de texto a 'category' es una de las mayores ganancias de memoria.\n",
    "for col in df_model.select_dtypes(include=['object']).columns:\n",
    "    # Solo convertimos a categoría si no es una columna de vector que vamos a parsear\n",
    "    if not df_model[col].str.contains(r'\\[', regex=True, na=False).any():\n",
    "        print(f\"Convirtiendo '{col}' a tipo 'category'...\")\n",
    "        df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# --- B. Parseo y Expansión de Vectores ---\n",
    "print(\"Buscando y expandiendo columnas de vectores...\")\n",
    "vector_like_columns = [col for col in df_model.columns if df_model[col].dtype == 'object' and df_model[col].astype(str).str.contains(r'\\[', regex=True, na=False).any()]\n",
    "\n",
    "for col_name in vector_like_columns:\n",
    "    # (El código para expandir vectores sigue igual que en la versión anterior)\n",
    "    print(f\"Procesando y expandiendo '{col_name}'...\")\n",
    "    extracted = df_model[col_name].fillna('').astype(str).str.findall(r'(-?\\d+\\.?\\d*e?-?\\d+)')\n",
    "    temp_df = pd.DataFrame(extracted.tolist(), index=df_model.index).astype(float)\n",
    "    if not temp_df.empty:\n",
    "        num_components = temp_df.shape[1]\n",
    "        temp_df.columns = [f\"{col_name}_{i}\" for i in range(num_components)]\n",
    "        df_model = df_model.join(temp_df)\n",
    "    df_model.drop(columns=[col_name], inplace=True)\n",
    "\n",
    "# --- C. Definición Final de Features y Preprocesamiento ---\n",
    "# (El resto del preprocesamiento es similar a la versión anterior)\n",
    "target = 'is_cheater'\n",
    "numerical_features = [col for col in df_model.columns if pd.api.types.is_numeric_dtype(df_model[col].dtype) and col != target]\n",
    "categorical_features = [col for col in df_model.columns if isinstance(df_model[col].dtype, pd.CategoricalDtype)]\n",
    "\n",
    "for col in numerical_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        df_model[col].fillna(df_model[col].median(), inplace=True)\n",
    "\n",
    "if categorical_features:\n",
    "    df_model = pd.get_dummies(df_model, columns=categorical_features, drop_first=True)\n",
    "\n",
    "X = df_model.drop(columns=[target])\n",
    "y = df_model[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Preprocesamiento completado.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 3: PREPROCESAMIENTO FINAL\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 3: Preprocesamiento final...\")\n",
    "\n",
    "# --- A. Manejo de Valores Faltantes ---\n",
    "print(\"Manejando valores faltantes...\")\n",
    "for col in numerical_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        median_val = df_model[col].median()\n",
    "        df_model[col].fillna(median_val, inplace=True)\n",
    "\n",
    "for col in categorical_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        df_model[col].fillna('missing', inplace=True)\n",
    "\n",
    "# --- B. One-Hot Encoding ---\n",
    "if categorical_features:\n",
    "    print(\"Aplicando One-Hot Encoding...\")\n",
    "    df_model = pd.get_dummies(df_model, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# --- C. Separación de Datos ---\n",
    "print(\"Separando datos...\")\n",
    "X = df_model.drop(columns=[target])\n",
    "y = df_model[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- D. Escalado ---\n",
    "print(\"Escalando características...\")\n",
    "# Guardamos los nombres de las columnas para poder reconstruir el DataFrame más tarde si es necesario\n",
    "X_train_cols = X_train.columns\n",
    "X_test_cols = X_test.columns\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Preprocesamiento completado.\")\n",
    "\n",
    "# ... (El resto del código para construir, entrenar y evaluar el modelo sigue igual) ...\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 4: CONSTRUCCIÓN DEL MODELO DE DEEP LEARNING\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 4: Construyendo el modelo de red neuronal...\")\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])\n",
    "model.summary()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 5: ENTRENAMIENTO DEL MODELO\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 5: Entrenando el modelo...\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=1024,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1,\n",
    "                    callbacks=[reduce_lr])\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PASO 6: EVALUACIÓN DEL MODELO\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\nPaso 6: Evaluando el rendimiento del modelo...\")\n",
    "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "predictions_prob = model.predict(X_test)\n",
    "binary_predictions = (predictions_prob > 0.5).astype(int)\n",
    "print(f'\\nResultados en el conjunto de prueba:')\n",
    "print(f'Pérdida (Loss): {loss:.4f}')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, binary_predictions))\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
